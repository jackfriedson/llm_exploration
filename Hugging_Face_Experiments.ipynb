{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T20:36:24.250889107Z",
     "start_time": "2024-02-07T20:36:17.423906108Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ermEpXryoa41",
    "outputId": "f6257f5b-51d8-4b35-d7f2-b3869a35b7f9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LrhBre7STsY",
    "outputId": "9cfe9ecf-a708-4a0a-dc15-c1c66b4e8b5f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "squad2 = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmMlfVDMxBH2"
   },
   "source": [
    "## Pre-process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2m8dHx4nqNrh"
   },
   "outputs": [],
   "source": [
    "MAX_TOKEN_LENGTH = 512\n",
    "STRIDE = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_token_length=MAX_TOKEN_LENGTH,\n",
    "    stride=STRIDE,\n",
    "):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_token_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "\n",
    "        # If the answer doesn't exist, the label is (0, 0)\n",
    "        if not answer[\"answer_start\"]:\n",
    "          start_positions.append(0)\n",
    "          end_positions.append(0)\n",
    "          continue\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fsA2fUKdrELG"
   },
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# train_dataset = squad2[\"train\"].select(range(5000))\n",
    "# preprocessed_train_dataset = train_dataset.map(\n",
    "#     partial(preprocess_training_examples, tokenizer=roberta_squad2_tokenizer),\n",
    "#     batched=True,\n",
    "#     remove_columns=train_dataset.column_names,\n",
    "# )\n",
    "# len(train_dataset), len(preprocessed_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3lJHDK_xI70"
   },
   "source": [
    "## Pre-process Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q_MguFc1xLjj"
   },
   "outputs": [],
   "source": [
    "def preprocess_test_examples(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_token_length=MAX_TOKEN_LENGTH,\n",
    "    stride=STRIDE,\n",
    "):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_token_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljtD3U0tGt7a"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tQKc9nZzs8M4"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "\n",
    "BATCH_SIZE = 700\n",
    "\n",
    "\n",
    "def infer_outputs(model, preprocessed_inputs, batch_size=BATCH_SIZE):\n",
    "    preprocessed_inputs.set_format(\"torch\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"No GPU available, using CPU for inference\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    num_examples = len(preprocessed_inputs)\n",
    "    outputs = None\n",
    "    for batch_start in trange(0, num_examples, batch_size):\n",
    "      batch_end = min(batch_start+batch_size, num_examples)\n",
    "      batch_inputs = preprocessed_inputs.select(range(batch_start, batch_end))\n",
    "      batch_inputs_for_model = {\n",
    "          k: batch_inputs[k].to(device)\n",
    "          for k in batch_inputs.column_names\n",
    "          if k not in [\"offset_mapping\", \"example_id\"]\n",
    "      }\n",
    "\n",
    "      with torch.no_grad():\n",
    "        batch_outputs = model(**batch_inputs_for_model)\n",
    "\n",
    "      # Free memory for inputs\n",
    "      for v in batch_inputs_for_model.values():\n",
    "        del v\n",
    "\n",
    "      if outputs is None:\n",
    "        outputs = batch_outputs\n",
    "        for v in outputs.values():\n",
    "          v.cpu()\n",
    "      else:\n",
    "        for k in batch_outputs.keys():\n",
    "          outputs[k] = torch.cat((outputs[k], batch_outputs[k]), dim=0)\n",
    "\n",
    "      # Free memory for outputs\n",
    "      for v in batch_outputs.values():\n",
    "        del v\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmWfO0hnzSAw"
   },
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3OSyutP-QjI2"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def post_process(\n",
    "    outputs,\n",
    "    test_dataset,\n",
    "    preprocessed_test_dataset,\n",
    "    n_best=20,\n",
    "    max_answer_length=30,\n",
    "):\n",
    "    preprocessed_test_dataset.set_format()\n",
    "\n",
    "    start_logits = outputs.start_logits.cpu().to(torch.float32).numpy()\n",
    "    end_logits = outputs.end_logits.cpu().to(torch.float32).numpy()\n",
    "    offset_mapping = preprocessed_test_dataset[\"offset_mapping\"]\n",
    "\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    example_ids = preprocessed_test_dataset[\"example_id\"]\n",
    "    for idx, example_id in tqdm.tqdm(enumerate(example_ids), total=len(example_ids)):\n",
    "        example_to_features[example_id].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm.tqdm(test_dataset, total=len(test_dataset)):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = offset_mapping[feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Prediction is that there's no answer\n",
    "                    if start_index == 0 and end_index == 0:\n",
    "                      answers.append(\n",
    "                          {\n",
    "                              \"text\": None,\n",
    "                              \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                          }\n",
    "                      )\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    elif offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    elif (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "                    else:\n",
    "                      answers.append(\n",
    "                          {\n",
    "                              \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                              \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                          }\n",
    "                      )\n",
    "\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        predicted_answers.append(\n",
    "            {\n",
    "                \"id\": example_id,\n",
    "                \"prediction_text\": best_answer[\"text\"] or \"\",\n",
    "                # TODO: can this be improved?\n",
    "                \"no_answer_probability\": 1.0 if best_answer[\"text\"] else 0.0\n",
    "            }\n",
    "        )\n",
    "    return predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VZt2AlyWYZpG"
   },
   "outputs": [],
   "source": [
    "def find_examples_with_no_answer(dataset):\n",
    "  example_idxs = []\n",
    "  for idx, ex in enumerate(dataset):\n",
    "    if not ex[\"answers\"].get(\"text\"):\n",
    "      example_idxs.append(idx)\n",
    "  return example_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IOboLEZwWvKe"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "41cyRvaqqEyb"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def evaluate_model(model_name, test_dataset, inference_batch_size=500):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    preprocessed_test_dataset = test_dataset.map(\n",
    "        partial(preprocess_test_examples, tokenizer=tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=test_dataset.column_names,\n",
    "    )\n",
    "\n",
    "    outputs = infer_outputs(model, preprocessed_test_dataset, batch_size=inference_batch_size)\n",
    "\n",
    "    predicted_answers = post_process(\n",
    "        outputs,\n",
    "        test_dataset,\n",
    "        preprocessed_test_dataset=preprocessed_test_dataset,\n",
    "    )\n",
    "    theoretical_answers = [\n",
    "        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_dataset\n",
    "    ]\n",
    "\n",
    "    print(metric.compute(predictions=predicted_answers, references=theoretical_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gyN05X4npf4b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:06<00:00,  4.18s/it]\n",
      "100%|██████████| 11955/11955 [00:00<00:00, 720961.96it/s]\n",
      "100%|██████████| 11873/11873 [00:04<00:00, 2845.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 79.91240630000843, 'f1': 82.91556320321106, 'total': 11873, 'HasAns_exact': 77.9689608636977, 'HasAns_f1': 83.98388696216712, 'HasAns_total': 5928, 'NoAns_exact': 81.85029436501262, 'NoAns_f1': 81.85029436501262, 'NoAns_total': 5945, 'best_exact': 79.91240630000843, 'best_exact_thresh': 1.0, 'best_f1': 82.91556320321097, 'best_f1_thresh': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RoBERTa\n",
    "evaluate_model(\"deepset/roberta-base-squad2\", squad2[\"validation\"], inference_batch_size=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGQ-SvKvd5TT",
    "outputId": "82f84f71-e735-49a3-ea74-c35626b67310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [02:28<00:00,  4.24s/it]\n",
      "100%|██████████| 12054/12054 [00:00<00:00, 821963.30it/s]\n",
      "100%|██████████| 11873/11873 [00:05<00:00, 2063.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 80.30826244420113, 'f1': 83.47166598148709, 'total': 11873, 'HasAns_exact': 79.53778677462888, 'HasAns_f1': 85.87366568795501, 'HasAns_total': 5928, 'NoAns_exact': 81.07653490328006, 'NoAns_f1': 81.07653490328006, 'NoAns_total': 5945, 'best_exact': 80.30826244420113, 'best_exact_thresh': 1.0, 'best_f1': 83.47166598148702, 'best_f1_thresh': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MDeBERTa-v3\n",
    "evaluate_model(\"timpal0l/mdeberta-v3-base-squad2\", squad2[\"validation\"], inference_batch_size=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa0187dba1846e6ab3dafcfebe903db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69b614fc2fe42898dfa832f173346dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/326M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e279f37cb724ae5b818f6565f0f08c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad903682717437088a53e073974929d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e50aeb1252448bb5523fae280c4c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d7fd449cc2463f8261bfd308a58371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a826e628a8df43e9a16383988ab082ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3281e1615da4432861a579cab0037b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:32<00:00,  2.73s/it]\n",
      "100%|██████████| 11955/11955 [00:00<00:00, 790673.06it/s]\n",
      "100%|██████████| 11873/11873 [00:04<00:00, 2472.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 78.93539964625622, 'f1': 81.97449894113623, 'total': 11873, 'HasAns_exact': 76.48448043184885, 'HasAns_f1': 82.57139438733313, 'HasAns_total': 5928, 'NoAns_exact': 81.37931034482759, 'NoAns_f1': 81.37931034482759, 'NoAns_total': 5945, 'best_exact': 78.93539964625622, 'best_exact_thresh': 1.0, 'best_f1': 81.97449894113613, 'best_f1_thresh': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate tiny RoBERTa\n",
    "evaluate_model(\"deepset/tinyroberta-squad2\", squad2[\"validation\"], inference_batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7e0d1bb3284aca9ec0450165be4f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0271ae4b764811b04caaa17c8d94b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8879a2aa774e0b83e4e358062ed251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ee62a1dcce4ac7a055d36ed46d4d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7f7f3373fd42ed91a03b15bc0be9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6435a9a07f0e48c4b0bb55ec8f141fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:32<00:00,  2.70s/it]\n",
      "100%|██████████| 11974/11974 [00:00<00:00, 1193332.61it/s]\n",
      "100%|██████████| 11873/11873 [00:03<00:00, 3134.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact': 39.84671102501474, 'f1': 43.62804078501804, 'total': 11873, 'HasAns_exact': 79.53778677462888, 'HasAns_f1': 87.11129018902147, 'HasAns_total': 5928, 'NoAns_exact': 0.2691337258200168, 'NoAns_f1': 0.2691337258200168, 'NoAns_total': 5945, 'best_exact': 50.11370336056599, 'best_exact_thresh': 1.0, 'best_f1': 50.11370336056599, 'best_f1_thresh': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate distilbert\n",
    "evaluate_model(\"distilbert/distilbert-base-cased-distilled-squad\", squad2[\"validation\"], inference_batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
