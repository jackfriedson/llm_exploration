{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook largely follows the [Hugging Face Question Answering tutorial](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt), with some adjustments (e.g. using squad-v2 instead of squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T20:36:24.250889107Z",
     "start_time": "2024-02-07T20:36:17.423906108Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ermEpXryoa41",
    "outputId": "f6257f5b-51d8-4b35-d7f2-b3869a35b7f9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/datadog/apm/library/python/ddtrace_pkgs/site-packages-ddtrace-py3.10-manylinux2014 (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/datadog/apm/library/python/ddtrace_pkgs/site-packages-ddtrace-py3.10-manylinux2014 (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/datadog/apm/library/python/ddtrace_pkgs/site-packages-ddtrace-py3.10-manylinux2014 (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for protobuf: [Errno 13] Permission denied: '/opt/datadog/apm/library/python/ddtrace_pkgs/site-packages-ddtrace-py3.10-manylinux2014/protobuf-4.25.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install torch transformers transformers[torch] datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LrhBre7STsY",
    "outputId": "9cfe9ecf-a708-4a0a-dc15-c1c66b4e8b5f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "squad2 = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmMlfVDMxBH2"
   },
   "source": [
    "## Pre-process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2m8dHx4nqNrh"
   },
   "outputs": [],
   "source": [
    "MAX_TOKEN_LENGTH = 512\n",
    "STRIDE = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_token_length=MAX_TOKEN_LENGTH,\n",
    "    stride=STRIDE,\n",
    "):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_token_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "\n",
    "        # If the answer doesn't exist, the label is (0, 0)\n",
    "        if not answer[\"answer_start\"]:\n",
    "          start_positions.append(0)\n",
    "          end_positions.append(0)\n",
    "          continue\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3lJHDK_xI70"
   },
   "source": [
    "## Pre-process Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q_MguFc1xLjj"
   },
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_token_length=MAX_TOKEN_LENGTH,\n",
    "    stride=STRIDE,\n",
    "):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_token_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljtD3U0tGt7a"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tQKc9nZzs8M4"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "BATCH_SIZE = 700\n",
    "\n",
    "\n",
    "def do_inference(model, inputs, batch_size=BATCH_SIZE):\n",
    "    inputs.set_format(\"torch\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"No GPU available, using CPU for inference\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    num_examples = len(inputs)\n",
    "    outputs = None\n",
    "    for batch_start in trange(0, num_examples, batch_size, desc=\"Inference\"):\n",
    "      batch_end = min(batch_start+batch_size, num_examples)\n",
    "      batch_inputs = inputs.select(range(batch_start, batch_end))\n",
    "      batch_inputs_for_model = {\n",
    "          k: batch_inputs[k].to(device)\n",
    "          for k in batch_inputs.column_names\n",
    "          if k not in [\"offset_mapping\", \"example_id\"]\n",
    "      }\n",
    "\n",
    "      with torch.no_grad():\n",
    "        batch_outputs = model(**batch_inputs_for_model)\n",
    "\n",
    "      # Free memory for inputs\n",
    "      for v in batch_inputs_for_model.values():\n",
    "        del v\n",
    "\n",
    "      if outputs is None:\n",
    "        outputs = batch_outputs\n",
    "        for v in outputs.values():\n",
    "          v.cpu()\n",
    "      else:\n",
    "        for k in batch_outputs.keys():\n",
    "          outputs[k] = torch.cat((outputs[k], batch_outputs[k]), dim=0)\n",
    "\n",
    "      # Free memory for outputs\n",
    "      for v in batch_outputs.values():\n",
    "        del v\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmWfO0hnzSAw"
   },
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3OSyutP-QjI2"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def post_process(\n",
    "    start_logits,\n",
    "    end_logits,\n",
    "    validation_dataset,\n",
    "    validation_features,\n",
    "    n_best=20,\n",
    "    max_answer_length=30,\n",
    "):\n",
    "    validation_features.set_format()\n",
    "    offset_mapping = validation_features[\"offset_mapping\"]\n",
    "\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    example_ids = validation_features[\"example_id\"]\n",
    "    for idx, example_id in enumerate(example_ids):\n",
    "        example_to_features[example_id].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(validation_dataset, total=len(validation_dataset), desc=\"Post process\"):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = offset_mapping[feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Prediction is that there's no answer\n",
    "                    if start_index == 0 and end_index == 0:\n",
    "                      answers.append(\n",
    "                          {\n",
    "                              \"text\": None,\n",
    "                              \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                          }\n",
    "                      )\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    elif offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    elif (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "                    else:\n",
    "                      answers.append(\n",
    "                          {\n",
    "                              \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                              \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                          }\n",
    "                      )\n",
    "\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        predicted_answers.append(\n",
    "            {\n",
    "                \"id\": example_id,\n",
    "                \"prediction_text\": best_answer[\"text\"] or \"\",\n",
    "                # TODO: can this be improved?\n",
    "                \"no_answer_probability\": 1.0 if best_answer[\"text\"] else 0.0\n",
    "            }\n",
    "        )\n",
    "    return predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VZt2AlyWYZpG"
   },
   "outputs": [],
   "source": [
    "def find_examples_with_no_answer(dataset):\n",
    "  example_idxs = []\n",
    "  for idx, ex in enumerate(dataset):\n",
    "    if not ex[\"answers\"].get(\"text\"):\n",
    "      example_idxs.append(idx)\n",
    "  return example_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IOboLEZwWvKe"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "squad_v2_metric = evaluate.load(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "41cyRvaqqEyb"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    start_logits,\n",
    "    end_logits,\n",
    "    validation_dataset,\n",
    "    validation_features,\n",
    "    metric,\n",
    "):\n",
    "    predicted_answers = post_process(\n",
    "        start_logits,\n",
    "        end_logits,\n",
    "        validation_dataset=validation_dataset,\n",
    "        validation_features=validation_features,\n",
    "    )\n",
    "    theoretical_answers = [\n",
    "        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in validation_dataset\n",
    "    ]\n",
    "\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "        model_name=None,\n",
    "        model=None,\n",
    "        tokenizer=None,\n",
    "        validation_dataset=None,\n",
    "        inference_batch_size=500,\n",
    "):\n",
    "    if model_name:\n",
    "        if model is not None or tokenizer is not None:\n",
    "            raise ValueError(\"Cannot specify both model_name and model/tokenizer\")\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    elif model is None or tokenizer is None:\n",
    "        raise ValueError(\"Must specify either model_name or model and tokenizer\")\n",
    "\n",
    "    if validation_dataset is None:\n",
    "        raise ValueError(\"Must specify validation_dataset\")\n",
    "\n",
    "    validation_features = validation_dataset.map(\n",
    "        partial(preprocess_validation_examples, tokenizer=tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=validation_dataset.column_names,\n",
    "    )\n",
    "    outputs = do_inference(model, validation_features, batch_size=inference_batch_size)\n",
    "    \n",
    "    start_logits = outputs.start_logits.cpu().to(torch.float32).numpy()\n",
    "    end_logits = outputs.end_logits.cpu().to(torch.float32).numpy()\n",
    "    \n",
    "    metrics = compute_metrics(\n",
    "        start_logits,\n",
    "        end_logits,\n",
    "        validation_dataset=validation_dataset,\n",
    "        validation_features=validation_features,\n",
    "        metric=squad_v2_metric,\n",
    "    )\n",
    "    pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c0b2bbdbc44565bc083917347ac685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44520dace57a432b8962df3520dd65b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff617c60cbde411fa4dcea5f1671654a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Post process:   0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HasAns_exact': 79.53778677462888,\n",
      " 'HasAns_f1': 87.11129018902147,\n",
      " 'HasAns_total': 5928,\n",
      " 'NoAns_exact': 0.2691337258200168,\n",
      " 'NoAns_f1': 0.2691337258200168,\n",
      " 'NoAns_total': 5945,\n",
      " 'best_exact': 50.11370336056599,\n",
      " 'best_exact_thresh': 1.0,\n",
      " 'best_f1': 50.11370336056599,\n",
      " 'best_f1_thresh': 1.0,\n",
      " 'exact': 39.84671102501474,\n",
      " 'f1': 43.62804078501804,\n",
      " 'total': 11873}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate distilbert\n",
    "evaluate_model(\n",
    "    model_name=\"distilbert/distilbert-base-cased-distilled-squad\",\n",
    "    validation_dataset=squad2[\"validation\"],\n",
    "    inference_batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75908d26c4a84d5484b198fa84f6b8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7c50043ed944c3bf3303e5f13bf3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cb1be360e64036a24c187c225a93a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Post process:   0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HasAns_exact': 76.48448043184885,\n",
      " 'HasAns_f1': 82.57139438733313,\n",
      " 'HasAns_total': 5928,\n",
      " 'NoAns_exact': 81.37931034482759,\n",
      " 'NoAns_f1': 81.37931034482759,\n",
      " 'NoAns_total': 5945,\n",
      " 'best_exact': 78.93539964625622,\n",
      " 'best_exact_thresh': 1.0,\n",
      " 'best_f1': 81.97449894113613,\n",
      " 'best_f1_thresh': 1.0,\n",
      " 'exact': 78.93539964625622,\n",
      " 'f1': 81.97449894113623,\n",
      " 'total': 11873}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate tiny RoBERTa\n",
    "evaluate_model(\n",
    "    model_name=\"deepset/tinyroberta-squad2\",\n",
    "    validation_dataset=squad2[\"validation\"],\n",
    "    inference_batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dd383a84cf4263abfbac401f11bbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6d30ebde7e45feba732d5eb66631fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/104255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a30981d5182427a84700b59f1d61adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a9f0483f9747618600c6aa2b9b2863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 104255, features: 104439\n",
      "Test examples: 26064, features: 26105\n",
      "Validation examples: 11873, features: 11974\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_test_dataset = squad2[\"train\"].train_test_split(test_size=0.1)\n",
    "shuffle_seed = 999999\n",
    "\n",
    "raw_train_dataset = train_test_dataset[\"train\"]\n",
    "train_dataset = raw_train_dataset.map(\n",
    "    partial(preprocess_training_examples, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=raw_train_dataset.column_names,\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(seed=shuffle_seed)\n",
    "\n",
    "raw_test_dataset = train_test_dataset[\"test\"]\n",
    "test_dataset = raw_test_dataset.map(\n",
    "    partial(preprocess_training_examples, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=raw_test_dataset.column_names,\n",
    ")\n",
    "test_dataset = test_dataset.shuffle(seed=shuffle_seed)\n",
    "\n",
    "raw_validation_dataset = squad2[\"validation\"]\n",
    "validation_dataset = raw_validation_dataset.map(\n",
    "    partial(preprocess_validation_examples, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=raw_validation_dataset.column_names,\n",
    ")\n",
    "validation_dataset = validation_dataset.shuffle(seed=shuffle_seed)\n",
    "\n",
    "print(f\"Train examples: {len(raw_train_dataset)}, features: {len(train_dataset)}\")\n",
    "print(f\"Test examples: {len(raw_test_dataset)}, features: {len(test_dataset)}\")\n",
    "print(f\"Validation examples: {len(raw_validation_dataset)}, features: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"models/distilbert-base-cased-distilled-squad-v2\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39165' max='39165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39165/39165 1:35:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3917</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>0.893741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7834</td>\n",
       "      <td>0.914200</td>\n",
       "      <td>0.835824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11751</td>\n",
       "      <td>0.890600</td>\n",
       "      <td>0.792013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15668</td>\n",
       "      <td>0.640500</td>\n",
       "      <td>0.840566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19585</td>\n",
       "      <td>0.647400</td>\n",
       "      <td>0.802553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23502</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>0.786321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27419</td>\n",
       "      <td>0.405200</td>\n",
       "      <td>0.905506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31336</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35253</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.927741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory models/distilbert-base-cased-distilled-squad-v2/checkpoint-13055 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-cased-distilled-squad-v2/checkpoint-26110 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-cased-distilled-squad-v2/checkpoint-39165 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39165, training_loss=0.6611109690359995, metrics={'train_runtime': 5707.85, 'train_samples_per_second': 54.892, 'train_steps_per_second': 6.862, 'total_flos': 4.093583734272614e+16, 'train_loss': 0.6611109690359995, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jackfriedson/distilbert-base-cased-distilled-squad-v2/commit/f8ebe183c921dcc57516df779de92bf2aa9c7c8e', commit_message='Training complete', commit_description='', oid='f8ebe183c921dcc57516df779de92bf2aa9c7c8e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701caf14e11c47bcb58e32dad54c3189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Post process:   0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact': 66.88284342626126,\n",
       " 'f1': 70.26828959712651,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 66.21120107962213,\n",
       " 'HasAns_f1': 72.99180202204461,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 67.55256518082422,\n",
       " 'NoAns_f1': 67.55256518082422,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 66.88284342626126,\n",
       " 'best_exact_thresh': 1.0,\n",
       " 'best_f1': 70.26828959712655,\n",
       " 'best_f1_thresh': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = compute_metrics(\n",
    "    start_logits,\n",
    "    end_logits,\n",
    "    validation_dataset=raw_validation_dataset,\n",
    "    validation_features=validation_dataset,\n",
    "    metric=squad_v2_metric,\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
